{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Environment Dynamics in Partially Observable and Multi-agent Settings with Feed-forward and Recurrent Networks\n",
    "\n",
    "## Single agent environments with partial observability\n",
    "\n",
    "Dependencies: OpenAI Gym, fancy impute, keras, mujoco (to run the Swimmer and Hopper environments -- otherwise BipedalWalker-v2 can be run)\n",
    "\n",
    "After downloading the saved replay memories from \n",
    "\n",
    "https://drive.google.com/open?id=1qS4mQmf4KFUVcO8RJMgpUZ5XIvGKm--M\n",
    "\n",
    "replace the env_name and path in \n",
    "\n",
    "np.load('../save_memory/{}IMPUTED0.25round0.npy'.format(env_name))\n",
    "\n",
    "below, with the path in which the memory to be used is stored. Then you can run the code as follows.\n",
    "\n",
    "A tensorboard log is written to \"./out/dynamics_learning/....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDP_learning.single_agent.dynamics_learning as ml\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env_name = \"BipedalWalker-v2\"\n",
    "env = gym.make(env_name)\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "ML = ml.ModelLearner(env_name, observation_space, action_space, partial_obs_rate=0.25, sequence_length=3, epochs=10)\n",
    "mem = np.load('./save_memory/{}IMPUTED0.25round0.npy'.format(env_name))\n",
    "ml.standardise_memory(mem, ML.state_size, ML.action_size)\n",
    "ML.memory = mem\n",
    "ML.train_models(minibatch_size=None, steps_per_epoch=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent environment\n",
    "Dependencies: OpenAI multi-agent environments (https://github.com/openai/multiagent-particle-envs)\n",
    "\n",
    "Tensorboard log will be written to \"./out/multi/<MultiAgentEnv instance>...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDP_learning.multi_agent import multi, make_env2\n",
    "\n",
    "env_name = 'simple'\n",
    "env = make_env2.make_env(env_name)\n",
    "\n",
    "# Sequence length of 0 uses a feed-forward network\n",
    "MAML = multi.MultiAgentModelLearner(env, mem_size=100000, sequence_length=0, scenario_name=env_name, epochs=100)\n",
    "MAML.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
