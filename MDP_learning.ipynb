{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Environment Dynamics in Partially Observable and Multi-agent Settings with Feed-forward and Recurrent Networks\n",
    "\n",
    "## Single agent environments with partial observability\n",
    "\n",
    "Dependencies: OpenAI Gym, fancy impute, keras, mujoco\n",
    "\n",
    "Tensorboard log is written to \"./out/dynamics_learning/....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDP_learning.single_agent.dynamics_learning as ml\n",
    "import gym\n",
    "\n",
    "env_name = \"Hopper-v2\"\n",
    "env = gym.make(env_name)\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "ML = ml.ModelLearner(env_name, observation_space, action_space, partial_obs_rate=0.25, sequence_length=3, epochs=10)\n",
    "ML.run(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent environment\n",
    "Dependencies: OpenAI multi-agent environments (https://github.com/openai/multiagent-particle-envs)\n",
    "\n",
    "Tensorboard log will be written to \"./out/multi/<MultiAgentEnv instance>...\"\n",
    "\n",
    "Try different sequence_lengths to see the difference in providing more information, i.e. more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from MDP_learning.multi_agent import multi, make_env2\n",
    "\n",
    "env_name = 'simple'\n",
    "env = make_env2.make_env(env_name)\n",
    "\n",
    "# Sequence length of 0 uses a feed-forward network\n",
    "MAML = multi.MultiAgentModelLearner(env, mem_size=100000, sequence_length=100, scenario_name=env_name, epochs=100)\n",
    "MAML.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATARI environment\n",
    "Dependencies: https://github.com/keras-rl/keras-rl\n",
    "\n",
    "Tensorboard log will be written to \"./dqn_logs/...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDP_learning.from_pixels import dqn_kerasrl_modellearn\n",
    "import gym\n",
    "\n",
    "env_name = 'PongDeterministic-v4'\n",
    "cfg = dqn_kerasrl_modellearn.AtariConfig(env_name)\n",
    "environment = gym.make(cfg.env_name)\n",
    "print('Playing: {}'.format(environment))\n",
    "num_actions = environment.action_space.n\n",
    "\n",
    "processor = dqn_kerasrl_modellearn.AtariProcessor(cfg.INPUT_SHAPE)\n",
    "dqn_agent, hidden_state_size = dqn_kerasrl_modellearn.setupDQN(cfg, num_actions, processor)\n",
    "\n",
    "dqn_kerasrl_modellearn.trainDQN(cfg, environment, dqn_agent)\n",
    "\n",
    "for seq_len in [1, 4, 16]:\n",
    "    dynamics_model, dqn_convolutions = dqn_kerasrl_modellearn.trainML(\n",
    "        cfg, dqn_agent,\n",
    "        sequence_length=seq_len,\n",
    "        hstate_size=hidden_state_size,\n",
    "        layer_width=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
