{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Environment Dynamics in Partially Observable and Multi-agent Settings with Feed-forward and Recurrent Networks\n",
    "\n",
    "## Single agent environments with partial observability\n",
    "\n",
    "Dependencies: OpenAI Gym, fancy impute, keras, mujoco\n",
    "\n",
    "Tensorboard log is written to \"./out/dynamics_learning/....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDP_learning.single_agent.dynamics_learning as ml\n",
    "import gym\n",
    "\n",
    "env_name = \"Hopper-v2\"\n",
    "env = gym.make(env_name)\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "ML = ml.ModelLearner(env_name, observation_space, action_space, partial_obs_rate=0.25, sequence_length=3, epochs=10)\n",
    "ML.run(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent environment\n",
    "Dependencies: OpenAI multi-agent environments (https://github.com/openai/multiagent-particle-envs)\n",
    "\n",
    "Tensorboard log will be written to \"./out/multi/<MultiAgentEnv instance>...\"\n",
    "\n",
    "Try different sequence_lengths to see the difference in providing more information, i.e. more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: ./out/multi/<MultiAgentEnv instance>_18-07-10_21:06_seqlen100__scenario_namesimple/logs/Xlearn\n",
      "Logging to: ./out/ModelLearner/<MultiAgentEnv instance>_18-07-10_21:07_seqlen100__agentID0_scenario_simple_netDepth2/logs/Xlearn\n",
      "Building regression model dimMulti (17, 17) actiFuncs None LR 0.001 decay 0 clipNorm 0 recurrent True numHiddenlayers1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 51)          11220     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 51)                21012     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 104       \n",
      "=================================================================\n",
      "Total params: 32,336\n",
      "Trainable params: 32,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Saved ./out/ModelLearner/<MultiAgentEnv instance>_18-07-10_21:07_seqlen100__agentID0_scenario_simple_netDepth2/model0 to model\n",
      "Done filling the data!\n",
      "Train on 899 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "899/899 [==============================] - 4s 4ms/step - loss: 3.9902 - mean_squared_error: 3.9902 - mean_absolute_error: 1.4240 - COD: 440.5254 - NRMSE: 0.1785 - Rsquared: -0.0080 - val_loss: 5.0723 - val_mean_squared_error: 5.0723 - val_mean_absolute_error: 1.5502 - val_COD: 474.3136 - val_NRMSE: 0.2314 - val_Rsquared: -0.0678\n",
      "Epoch 2/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.9063 - mean_squared_error: 3.9063 - mean_absolute_error: 1.4137 - COD: 329.9727 - NRMSE: 0.1798 - Rsquared: -0.0040 - val_loss: 5.0937 - val_mean_squared_error: 5.0937 - val_mean_absolute_error: 1.5369 - val_COD: 530.9419 - val_NRMSE: 0.2291 - val_Rsquared: -0.0489\n",
      "Epoch 3/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.8140 - mean_squared_error: 3.8140 - mean_absolute_error: 1.4084 - COD: 144.2320 - NRMSE: 0.1756 - Rsquared: 0.0205 - val_loss: 5.5708 - val_mean_squared_error: 5.5708 - val_mean_absolute_error: 1.5990 - val_COD: 113.4001 - val_NRMSE: 0.2437 - val_Rsquared: -0.1857\n",
      "Epoch 4/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.7303 - mean_squared_error: 3.7303 - mean_absolute_error: 1.3960 - COD: 86.3674 - NRMSE: 0.1727 - Rsquared: 0.0539 - val_loss: 4.7608 - val_mean_squared_error: 4.7608 - val_mean_absolute_error: 1.5122 - val_COD: 110.7138 - val_NRMSE: 0.2241 - val_Rsquared: -0.0027\n",
      "Epoch 5/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.7069 - mean_squared_error: 3.7069 - mean_absolute_error: 1.3918 - COD: 44.4623 - NRMSE: 0.1713 - Rsquared: 0.0576 - val_loss: 5.0806 - val_mean_squared_error: 5.0806 - val_mean_absolute_error: 1.5334 - val_COD: 46.1990 - val_NRMSE: 0.2286 - val_Rsquared: -0.0439\n",
      "Epoch 6/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.5879 - mean_squared_error: 3.5879 - mean_absolute_error: 1.3912 - COD: 16.6150 - NRMSE: 0.1681 - Rsquared: 0.0718 - val_loss: 5.4615 - val_mean_squared_error: 5.4615 - val_mean_absolute_error: 1.5446 - val_COD: 14.2960 - val_NRMSE: 0.2321 - val_Rsquared: -0.0833\n",
      "Epoch 7/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.6913 - mean_squared_error: 3.6913 - mean_absolute_error: 1.3930 - COD: 14.4096 - NRMSE: 0.1799 - Rsquared: 0.0350 - val_loss: 5.7011 - val_mean_squared_error: 5.7011 - val_mean_absolute_error: 1.5835 - val_COD: 16.8963 - val_NRMSE: 0.2389 - val_Rsquared: -0.1449\n",
      "Epoch 8/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.5678 - mean_squared_error: 3.5678 - mean_absolute_error: 1.3776 - COD: 11.9249 - NRMSE: 0.1745 - Rsquared: 0.0535 - val_loss: 5.5400 - val_mean_squared_error: 5.5400 - val_mean_absolute_error: 1.5091 - val_COD: 11.8305 - val_NRMSE: 0.2264 - val_Rsquared: -0.0419\n",
      "Epoch 9/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.4604 - mean_squared_error: 3.4604 - mean_absolute_error: 1.3618 - COD: 9.2246 - NRMSE: 0.1699 - Rsquared: 0.0924 - val_loss: 5.2128 - val_mean_squared_error: 5.2128 - val_mean_absolute_error: 1.4864 - val_COD: 17.3871 - val_NRMSE: 0.2229 - val_Rsquared: -0.0040\n",
      "Epoch 10/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.4786 - mean_squared_error: 3.4786 - mean_absolute_error: 1.3565 - COD: 7.8782 - NRMSE: 0.1644 - Rsquared: 0.1130 - val_loss: 5.6023 - val_mean_squared_error: 5.6023 - val_mean_absolute_error: 1.5239 - val_COD: 9.4023 - val_NRMSE: 0.2296 - val_Rsquared: -0.0678\n",
      "Epoch 11/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.4481 - mean_squared_error: 3.4481 - mean_absolute_error: 1.3406 - COD: 9.4179 - NRMSE: 0.1671 - Rsquared: 0.1090 - val_loss: 5.1244 - val_mean_squared_error: 5.1244 - val_mean_absolute_error: 1.4765 - val_COD: 10.2839 - val_NRMSE: 0.2187 - val_Rsquared: 0.0293\n",
      "Epoch 12/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.3988 - mean_squared_error: 3.3988 - mean_absolute_error: 1.3414 - COD: 6.2780 - NRMSE: 0.1651 - Rsquared: 0.1212 - val_loss: 6.1005 - val_mean_squared_error: 6.1005 - val_mean_absolute_error: 1.5567 - val_COD: 6.1891 - val_NRMSE: 0.2331 - val_Rsquared: -0.1143\n",
      "Epoch 13/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.5426 - mean_squared_error: 3.5426 - mean_absolute_error: 1.3724 - COD: 14.3947 - NRMSE: 0.1654 - Rsquared: 0.0824 - val_loss: 5.3037 - val_mean_squared_error: 5.3037 - val_mean_absolute_error: 1.4970 - val_COD: 9.7594 - val_NRMSE: 0.2234 - val_Rsquared: -0.0106\n",
      "Epoch 14/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.3640 - mean_squared_error: 3.3640 - mean_absolute_error: 1.3429 - COD: 8.4950 - NRMSE: 0.1686 - Rsquared: 0.1086 - val_loss: 5.0995 - val_mean_squared_error: 5.0995 - val_mean_absolute_error: 1.4933 - val_COD: 15.5968 - val_NRMSE: 0.2237 - val_Rsquared: -0.0069\n",
      "Epoch 15/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.3813 - mean_squared_error: 3.3813 - mean_absolute_error: 1.3333 - COD: 10.8471 - NRMSE: 0.1660 - Rsquared: 0.1250 - val_loss: 4.5656 - val_mean_squared_error: 4.5656 - val_mean_absolute_error: 1.4450 - val_COD: 13.9782 - val_NRMSE: 0.2159 - val_Rsquared: 0.0677\n",
      "Epoch 16/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.2762 - mean_squared_error: 3.2762 - mean_absolute_error: 1.3180 - COD: 6.8647 - NRMSE: 0.1615 - Rsquared: 0.1635 - val_loss: 4.1438 - val_mean_squared_error: 4.1438 - val_mean_absolute_error: 1.3818 - val_COD: 13.5889 - val_NRMSE: 0.2045 - val_Rsquared: 0.1623\n",
      "Epoch 17/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.2049 - mean_squared_error: 3.2049 - mean_absolute_error: 1.2961 - COD: 5.1496 - NRMSE: 0.1694 - Rsquared: 0.1547 - val_loss: 4.2517 - val_mean_squared_error: 4.2517 - val_mean_absolute_error: 1.4008 - val_COD: 11.6791 - val_NRMSE: 0.2077 - val_Rsquared: 0.1362\n",
      "Epoch 18/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.2745 - mean_squared_error: 3.2745 - mean_absolute_error: 1.3043 - COD: 5.1583 - NRMSE: 0.1676 - Rsquared: 0.1560 - val_loss: 4.2019 - val_mean_squared_error: 4.2019 - val_mean_absolute_error: 1.3953 - val_COD: 9.9521 - val_NRMSE: 0.2064 - val_Rsquared: 0.1471\n",
      "Epoch 19/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.4196 - mean_squared_error: 3.4196 - mean_absolute_error: 1.3406 - COD: 6.0783 - NRMSE: 0.1737 - Rsquared: 0.0975 - val_loss: 5.1464 - val_mean_squared_error: 5.1464 - val_mean_absolute_error: 1.4805 - val_COD: 10.7084 - val_NRMSE: 0.2218 - val_Rsquared: 0.0043\n",
      "Epoch 20/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.3113 - mean_squared_error: 3.3113 - mean_absolute_error: 1.3208 - COD: 8.0790 - NRMSE: 0.1690 - Rsquared: 0.1281 - val_loss: 5.1492 - val_mean_squared_error: 5.1492 - val_mean_absolute_error: 1.4847 - val_COD: 10.4877 - val_NRMSE: 0.2204 - val_Rsquared: 0.0158\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1749 - mean_squared_error: 3.1749 - mean_absolute_error: 1.2934 - COD: 7.1285 - NRMSE: 0.1613 - Rsquared: 0.1711 - val_loss: 4.2476 - val_mean_squared_error: 4.2476 - val_mean_absolute_error: 1.3946 - val_COD: 10.8857 - val_NRMSE: 0.2075 - val_Rsquared: 0.1385\n",
      "Epoch 22/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1216 - mean_squared_error: 3.1216 - mean_absolute_error: 1.2809 - COD: 5.8203 - NRMSE: 0.1632 - Rsquared: 0.1746 - val_loss: 4.2608 - val_mean_squared_error: 4.2608 - val_mean_absolute_error: 1.3989 - val_COD: 12.6721 - val_NRMSE: 0.2088 - val_Rsquared: 0.1283\n",
      "Epoch 23/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0617 - mean_squared_error: 3.0617 - mean_absolute_error: 1.2603 - COD: 4.8535 - NRMSE: 0.1572 - Rsquared: 0.2106 - val_loss: 4.7126 - val_mean_squared_error: 4.7126 - val_mean_absolute_error: 1.4381 - val_COD: 8.7075 - val_NRMSE: 0.2183 - val_Rsquared: 0.0463\n",
      "Epoch 24/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.0822 - mean_squared_error: 3.0822 - mean_absolute_error: 1.2629 - COD: 4.0115 - NRMSE: 0.1545 - Rsquared: 0.2097 - val_loss: 4.9236 - val_mean_squared_error: 4.9236 - val_mean_absolute_error: 1.4751 - val_COD: 8.0666 - val_NRMSE: 0.2274 - val_Rsquared: -0.0319\n",
      "Epoch 25/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.0922 - mean_squared_error: 3.0922 - mean_absolute_error: 1.2631 - COD: 4.0683 - NRMSE: 0.1591 - Rsquared: 0.1924 - val_loss: 4.6025 - val_mean_squared_error: 4.6025 - val_mean_absolute_error: 1.4324 - val_COD: 9.8701 - val_NRMSE: 0.2171 - val_Rsquared: 0.0574\n",
      "Epoch 26/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1319 - mean_squared_error: 3.1319 - mean_absolute_error: 1.2755 - COD: 5.2063 - NRMSE: 0.1632 - Rsquared: 0.1903 - val_loss: 4.7487 - val_mean_squared_error: 4.7487 - val_mean_absolute_error: 1.4558 - val_COD: 6.4440 - val_NRMSE: 0.2222 - val_Rsquared: 0.0105\n",
      "Epoch 27/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.2908 - mean_squared_error: 3.2908 - mean_absolute_error: 1.3060 - COD: 4.6124 - NRMSE: 0.1675 - Rsquared: 0.1501 - val_loss: 4.5869 - val_mean_squared_error: 4.5869 - val_mean_absolute_error: 1.4282 - val_COD: 8.4430 - val_NRMSE: 0.2184 - val_Rsquared: 0.0483\n",
      "Epoch 28/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.2291 - mean_squared_error: 3.2291 - mean_absolute_error: 1.2967 - COD: 5.2149 - NRMSE: 0.1667 - Rsquared: 0.1574 - val_loss: 5.1571 - val_mean_squared_error: 5.1571 - val_mean_absolute_error: 1.4767 - val_COD: 8.3973 - val_NRMSE: 0.2303 - val_Rsquared: -0.0608\n",
      "Epoch 29/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.4380 - mean_squared_error: 3.4380 - mean_absolute_error: 1.3375 - COD: 5.3836 - NRMSE: 0.1710 - Rsquared: 0.1101 - val_loss: 5.0195 - val_mean_squared_error: 5.0195 - val_mean_absolute_error: 1.5037 - val_COD: 9.1708 - val_NRMSE: 0.2287 - val_Rsquared: -0.0432\n",
      "Epoch 30/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1884 - mean_squared_error: 3.1884 - mean_absolute_error: 1.3055 - COD: 6.1259 - NRMSE: 0.1619 - Rsquared: 0.1747 - val_loss: 4.7025 - val_mean_squared_error: 4.7025 - val_mean_absolute_error: 1.4594 - val_COD: 10.2169 - val_NRMSE: 0.2223 - val_Rsquared: 0.0141\n",
      "Epoch 31/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.2272 - mean_squared_error: 3.2272 - mean_absolute_error: 1.2923 - COD: 6.6586 - NRMSE: 0.1662 - Rsquared: 0.1665 - val_loss: 4.6259 - val_mean_squared_error: 4.6259 - val_mean_absolute_error: 1.4176 - val_COD: 8.4527 - val_NRMSE: 0.2154 - val_Rsquared: 0.0700\n",
      "Epoch 32/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1915 - mean_squared_error: 3.1915 - mean_absolute_error: 1.2774 - COD: 6.6817 - NRMSE: 0.1620 - Rsquared: 0.1704 - val_loss: 4.8262 - val_mean_squared_error: 4.8262 - val_mean_absolute_error: 1.4172 - val_COD: 8.0291 - val_NRMSE: 0.2176 - val_Rsquared: 0.0480\n",
      "Epoch 33/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.1075 - mean_squared_error: 3.1075 - mean_absolute_error: 1.2646 - COD: 4.9699 - NRMSE: 0.1581 - Rsquared: 0.1929 - val_loss: 4.9214 - val_mean_squared_error: 4.9214 - val_mean_absolute_error: 1.4349 - val_COD: 6.7594 - val_NRMSE: 0.2174 - val_Rsquared: 0.0464\n",
      "Epoch 34/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.4370 - mean_squared_error: 3.4370 - mean_absolute_error: 1.3245 - COD: 5.6999 - NRMSE: 0.1694 - Rsquared: 0.1058 - val_loss: 5.3750 - val_mean_squared_error: 5.3750 - val_mean_absolute_error: 1.4824 - val_COD: 6.5479 - val_NRMSE: 0.2319 - val_Rsquared: -0.0787\n",
      "Epoch 35/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1724 - mean_squared_error: 3.1724 - mean_absolute_error: 1.2687 - COD: 4.0900 - NRMSE: 0.1588 - Rsquared: 0.1863 - val_loss: 5.2189 - val_mean_squared_error: 5.2189 - val_mean_absolute_error: 1.4738 - val_COD: 5.3853 - val_NRMSE: 0.2224 - val_Rsquared: 1.5831e-06\n",
      "Epoch 36/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0431 - mean_squared_error: 3.0431 - mean_absolute_error: 1.2358 - COD: 4.3833 - NRMSE: 0.1541 - Rsquared: 0.2103 - val_loss: 5.1920 - val_mean_squared_error: 5.1920 - val_mean_absolute_error: 1.4578 - val_COD: 6.9329 - val_NRMSE: 0.2205 - val_Rsquared: 0.0136\n",
      "Epoch 37/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0738 - mean_squared_error: 3.0738 - mean_absolute_error: 1.2365 - COD: 4.6133 - NRMSE: 0.1652 - Rsquared: 0.1987 - val_loss: 5.1260 - val_mean_squared_error: 5.1260 - val_mean_absolute_error: 1.4405 - val_COD: 5.3867 - val_NRMSE: 0.2188 - val_Rsquared: 0.0284\n",
      "Epoch 38/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0077 - mean_squared_error: 3.0077 - mean_absolute_error: 1.2345 - COD: 3.3478 - NRMSE: 0.1607 - Rsquared: 0.2056 - val_loss: 5.0175 - val_mean_squared_error: 5.0175 - val_mean_absolute_error: 1.4537 - val_COD: 6.1636 - val_NRMSE: 0.2229 - val_Rsquared: 0.0021\n",
      "Epoch 39/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.0996 - mean_squared_error: 3.0996 - mean_absolute_error: 1.2530 - COD: 4.6482 - NRMSE: 0.1606 - Rsquared: 0.2010 - val_loss: 5.2176 - val_mean_squared_error: 5.2176 - val_mean_absolute_error: 1.4501 - val_COD: 7.2716 - val_NRMSE: 0.2201 - val_Rsquared: 0.0159\n",
      "Epoch 40/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9145 - mean_squared_error: 2.9145 - mean_absolute_error: 1.2220 - COD: 4.1970 - NRMSE: 0.1575 - Rsquared: 0.2387 - val_loss: 5.1431 - val_mean_squared_error: 5.1431 - val_mean_absolute_error: 1.4425 - val_COD: 5.4972 - val_NRMSE: 0.2180 - val_Rsquared: 0.0337\n",
      "Epoch 41/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8339 - mean_squared_error: 2.8339 - mean_absolute_error: 1.2038 - COD: 3.4860 - NRMSE: 0.1518 - Rsquared: 0.2644 - val_loss: 5.4655 - val_mean_squared_error: 5.4655 - val_mean_absolute_error: 1.4433 - val_COD: 5.8739 - val_NRMSE: 0.2251 - val_Rsquared: -0.0313\n",
      "Epoch 42/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1834 - mean_squared_error: 3.1834 - mean_absolute_error: 1.2638 - COD: 4.9311 - NRMSE: 0.1620 - Rsquared: 0.1564 - val_loss: 4.8535 - val_mean_squared_error: 4.8535 - val_mean_absolute_error: 1.4591 - val_COD: 10.1575 - val_NRMSE: 0.2242 - val_Rsquared: -0.0051\n",
      "Epoch 43/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0335 - mean_squared_error: 3.0335 - mean_absolute_error: 1.2411 - COD: 4.2041 - NRMSE: 0.1586 - Rsquared: 0.2134 - val_loss: 5.1458 - val_mean_squared_error: 5.1458 - val_mean_absolute_error: 1.4473 - val_COD: 6.0962 - val_NRMSE: 0.2228 - val_Rsquared: -0.0055\n",
      "Epoch 44/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0906 - mean_squared_error: 3.0906 - mean_absolute_error: 1.2343 - COD: 3.3402 - NRMSE: 0.1597 - Rsquared: 0.2088 - val_loss: 6.2335 - val_mean_squared_error: 6.2335 - val_mean_absolute_error: 1.5429 - val_COD: 5.8336 - val_NRMSE: 0.2433 - val_Rsquared: -0.2009\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 3s 3ms/step - loss: 3.3813 - mean_squared_error: 3.3813 - mean_absolute_error: 1.2803 - COD: 4.0722 - NRMSE: 0.1689 - Rsquared: 0.0952 - val_loss: 5.3065 - val_mean_squared_error: 5.3065 - val_mean_absolute_error: 1.4903 - val_COD: 7.0507 - val_NRMSE: 0.2307 - val_Rsquared: -0.0688\n",
      "Epoch 46/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.2892 - mean_squared_error: 3.2892 - mean_absolute_error: 1.2893 - COD: 4.0962 - NRMSE: 0.1605 - Rsquared: 0.1485 - val_loss: 4.6492 - val_mean_squared_error: 4.6492 - val_mean_absolute_error: 1.4801 - val_COD: 8.0073 - val_NRMSE: 0.2268 - val_Rsquared: -0.0217\n",
      "Epoch 47/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.3569 - mean_squared_error: 3.3569 - mean_absolute_error: 1.2930 - COD: 4.5772 - NRMSE: 0.1705 - Rsquared: 0.1469 - val_loss: 4.2921 - val_mean_squared_error: 4.2921 - val_mean_absolute_error: 1.4270 - val_COD: 7.0186 - val_NRMSE: 0.2159 - val_Rsquared: 0.0729\n",
      "Epoch 48/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.9646 - mean_squared_error: 2.9646 - mean_absolute_error: 1.2514 - COD: 3.9141 - NRMSE: 0.1589 - Rsquared: 0.2282 - val_loss: 4.4731 - val_mean_squared_error: 4.4731 - val_mean_absolute_error: 1.4414 - val_COD: 9.7610 - val_NRMSE: 0.2193 - val_Rsquared: 0.0424\n",
      "Epoch 49/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9179 - mean_squared_error: 2.9179 - mean_absolute_error: 1.2263 - COD: 4.0110 - NRMSE: 0.1540 - Rsquared: 0.2465 - val_loss: 4.5362 - val_mean_squared_error: 4.5362 - val_mean_absolute_error: 1.4148 - val_COD: 9.0199 - val_NRMSE: 0.2166 - val_Rsquared: 0.0605\n",
      "Epoch 50/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9167 - mean_squared_error: 2.9167 - mean_absolute_error: 1.2120 - COD: 3.8591 - NRMSE: 0.1531 - Rsquared: 0.2421 - val_loss: 5.1439 - val_mean_squared_error: 5.1439 - val_mean_absolute_error: 1.4930 - val_COD: 6.9561 - val_NRMSE: 0.2321 - val_Rsquared: -0.0751\n",
      "Epoch 51/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8724 - mean_squared_error: 2.8724 - mean_absolute_error: 1.2229 - COD: 2.8442 - NRMSE: 0.1520 - Rsquared: 0.2646 - val_loss: 5.1129 - val_mean_squared_error: 5.1129 - val_mean_absolute_error: 1.4772 - val_COD: 5.6953 - val_NRMSE: 0.2321 - val_Rsquared: -0.0734\n",
      "Epoch 52/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8755 - mean_squared_error: 2.8755 - mean_absolute_error: 1.2161 - COD: 2.9885 - NRMSE: 0.1535 - Rsquared: 0.2469 - val_loss: 4.4908 - val_mean_squared_error: 4.4908 - val_mean_absolute_error: 1.4281 - val_COD: 7.5800 - val_NRMSE: 0.2168 - val_Rsquared: 0.0624\n",
      "Epoch 53/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7925 - mean_squared_error: 2.7925 - mean_absolute_error: 1.1980 - COD: 3.0375 - NRMSE: 0.1510 - Rsquared: 0.2640 - val_loss: 4.6631 - val_mean_squared_error: 4.6631 - val_mean_absolute_error: 1.4309 - val_COD: 8.5446 - val_NRMSE: 0.2205 - val_Rsquared: 0.0303\n",
      "Epoch 54/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7461 - mean_squared_error: 2.7461 - mean_absolute_error: 1.1908 - COD: 3.2271 - NRMSE: 0.1582 - Rsquared: 0.2020 - val_loss: 4.8460 - val_mean_squared_error: 4.8460 - val_mean_absolute_error: 1.4536 - val_COD: 8.1850 - val_NRMSE: 0.2243 - val_Rsquared: -0.0037\n",
      "Epoch 55/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0767 - mean_squared_error: 3.0767 - mean_absolute_error: 1.2571 - COD: 3.5341 - NRMSE: 0.1554 - Rsquared: 0.1920 - val_loss: 5.9975 - val_mean_squared_error: 5.9975 - val_mean_absolute_error: 1.6227 - val_COD: 6.9214 - val_NRMSE: 0.2544 - val_Rsquared: -0.2924\n",
      "Epoch 56/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.1759 - mean_squared_error: 3.1759 - mean_absolute_error: 1.2710 - COD: 3.9591 - NRMSE: 0.1597 - Rsquared: 0.1639 - val_loss: 5.5220 - val_mean_squared_error: 5.5220 - val_mean_absolute_error: 1.5077 - val_COD: 5.1972 - val_NRMSE: 0.2303 - val_Rsquared: -0.0707\n",
      "Epoch 57/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9706 - mean_squared_error: 2.9706 - mean_absolute_error: 1.2290 - COD: 3.7721 - NRMSE: 0.1592 - Rsquared: 0.1817 - val_loss: 5.0600 - val_mean_squared_error: 5.0600 - val_mean_absolute_error: 1.4622 - val_COD: 5.8709 - val_NRMSE: 0.2231 - val_Rsquared: -0.0012\n",
      "Epoch 58/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9884 - mean_squared_error: 2.9884 - mean_absolute_error: 1.2263 - COD: 5.1274 - NRMSE: 0.1545 - Rsquared: 0.2175 - val_loss: 4.9665 - val_mean_squared_error: 4.9665 - val_mean_absolute_error: 1.4602 - val_COD: 6.7447 - val_NRMSE: 0.2247 - val_Rsquared: -0.0098\n",
      "Epoch 59/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.9536 - mean_squared_error: 2.9536 - mean_absolute_error: 1.2045 - COD: 4.4530 - NRMSE: 0.1528 - Rsquared: 0.2537 - val_loss: 5.1940 - val_mean_squared_error: 5.1940 - val_mean_absolute_error: 1.4682 - val_COD: 6.6931 - val_NRMSE: 0.2229 - val_Rsquared: -0.0043\n",
      "Epoch 60/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9602 - mean_squared_error: 2.9602 - mean_absolute_error: 1.2007 - COD: 4.8238 - NRMSE: 0.1625 - Rsquared: 0.2396 - val_loss: 4.3113 - val_mean_squared_error: 4.3113 - val_mean_absolute_error: 1.4001 - val_COD: 8.1861 - val_NRMSE: 0.2145 - val_Rsquared: 0.0843\n",
      "Epoch 61/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0068 - mean_squared_error: 3.0068 - mean_absolute_error: 1.2118 - COD: 4.4798 - NRMSE: 0.1520 - Rsquared: 0.2389 - val_loss: 4.4489 - val_mean_squared_error: 4.4489 - val_mean_absolute_error: 1.3942 - val_COD: 7.6134 - val_NRMSE: 0.2149 - val_Rsquared: 0.0786\n",
      "Epoch 62/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9730 - mean_squared_error: 2.9730 - mean_absolute_error: 1.2016 - COD: 4.2248 - NRMSE: 0.1522 - Rsquared: 0.2297 - val_loss: 4.5376 - val_mean_squared_error: 4.5376 - val_mean_absolute_error: 1.4206 - val_COD: 6.8180 - val_NRMSE: 0.2193 - val_Rsquared: 0.0425\n",
      "Epoch 63/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9156 - mean_squared_error: 2.9156 - mean_absolute_error: 1.1864 - COD: 3.3915 - NRMSE: 0.1526 - Rsquared: 0.2557 - val_loss: 4.8414 - val_mean_squared_error: 4.8414 - val_mean_absolute_error: 1.4022 - val_COD: 5.9174 - val_NRMSE: 0.2151 - val_Rsquared: 0.0647\n",
      "Epoch 64/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8194 - mean_squared_error: 2.8194 - mean_absolute_error: 1.1692 - COD: 3.2286 - NRMSE: 0.1499 - Rsquared: 0.2790 - val_loss: 4.7305 - val_mean_squared_error: 4.7305 - val_mean_absolute_error: 1.4150 - val_COD: 4.7797 - val_NRMSE: 0.2198 - val_Rsquared: 0.0340\n",
      "Epoch 65/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7526 - mean_squared_error: 2.7526 - mean_absolute_error: 1.1625 - COD: 2.7575 - NRMSE: 0.1468 - Rsquared: 0.2930 - val_loss: 4.7767 - val_mean_squared_error: 4.7767 - val_mean_absolute_error: 1.4171 - val_COD: 4.9975 - val_NRMSE: 0.2165 - val_Rsquared: 0.0562\n",
      "Epoch 66/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7561 - mean_squared_error: 2.7561 - mean_absolute_error: 1.1779 - COD: 2.6154 - NRMSE: 0.1465 - Rsquared: 0.2985 - val_loss: 4.8842 - val_mean_squared_error: 4.8842 - val_mean_absolute_error: 1.4627 - val_COD: 4.7053 - val_NRMSE: 0.2211 - val_Rsquared: 0.0202\n",
      "Epoch 67/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7503 - mean_squared_error: 2.7503 - mean_absolute_error: 1.1695 - COD: 2.2265 - NRMSE: 0.1532 - Rsquared: 0.1494 - val_loss: 5.2973 - val_mean_squared_error: 5.2973 - val_mean_absolute_error: 1.4961 - val_COD: 4.3203 - val_NRMSE: 0.2310 - val_Rsquared: -0.0720\n",
      "Epoch 68/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.6508 - mean_squared_error: 2.6508 - mean_absolute_error: 1.1587 - COD: 2.3792 - NRMSE: 0.1513 - Rsquared: 0.3056 - val_loss: 4.7728 - val_mean_squared_error: 4.7728 - val_mean_absolute_error: 1.4539 - val_COD: 4.9597 - val_NRMSE: 0.2216 - val_Rsquared: 0.0183\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 3s 3ms/step - loss: 2.6714 - mean_squared_error: 2.6714 - mean_absolute_error: 1.1499 - COD: 2.7518 - NRMSE: 0.1435 - Rsquared: 0.3205 - val_loss: 5.2480 - val_mean_squared_error: 5.2480 - val_mean_absolute_error: 1.4717 - val_COD: 4.7083 - val_NRMSE: 0.2252 - val_Rsquared: -0.0243\n",
      "Epoch 70/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.6121 - mean_squared_error: 2.6121 - mean_absolute_error: 1.1374 - COD: 2.4067 - NRMSE: 0.1439 - Rsquared: 0.2983 - val_loss: 5.0835 - val_mean_squared_error: 5.0835 - val_mean_absolute_error: 1.4492 - val_COD: 4.4571 - val_NRMSE: 0.2223 - val_Rsquared: 0.0051\n",
      "Epoch 71/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.6726 - mean_squared_error: 2.6726 - mean_absolute_error: 1.1610 - COD: 2.4035 - NRMSE: 0.1490 - Rsquared: 0.3070 - val_loss: 5.0193 - val_mean_squared_error: 5.0193 - val_mean_absolute_error: 1.4576 - val_COD: 4.3244 - val_NRMSE: 0.2242 - val_Rsquared: -0.0080\n",
      "Epoch 72/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7464 - mean_squared_error: 2.7464 - mean_absolute_error: 1.1438 - COD: 1.9396 - NRMSE: 0.1478 - Rsquared: 0.3016 - val_loss: 5.5044 - val_mean_squared_error: 5.5044 - val_mean_absolute_error: 1.5657 - val_COD: 3.9786 - val_NRMSE: 0.2473 - val_Rsquared: -0.2228\n",
      "Epoch 73/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8907 - mean_squared_error: 2.8907 - mean_absolute_error: 1.1853 - COD: 2.8165 - NRMSE: 0.1569 - Rsquared: 0.1304 - val_loss: 5.4066 - val_mean_squared_error: 5.4066 - val_mean_absolute_error: 1.4813 - val_COD: 4.2419 - val_NRMSE: 0.2323 - val_Rsquared: -0.0983\n",
      "Epoch 74/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7902 - mean_squared_error: 2.7902 - mean_absolute_error: 1.1669 - COD: 2.4698 - NRMSE: 0.1504 - Rsquared: 0.2975 - val_loss: 4.7264 - val_mean_squared_error: 4.7264 - val_mean_absolute_error: 1.4288 - val_COD: 3.4926 - val_NRMSE: 0.2221 - val_Rsquared: 0.0117\n",
      "Epoch 75/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7063 - mean_squared_error: 2.7063 - mean_absolute_error: 1.1417 - COD: 2.2573 - NRMSE: 0.1502 - Rsquared: 0.3013 - val_loss: 5.4401 - val_mean_squared_error: 5.4401 - val_mean_absolute_error: 1.4905 - val_COD: 3.6713 - val_NRMSE: 0.2349 - val_Rsquared: -0.1071\n",
      "Epoch 76/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8144 - mean_squared_error: 2.8144 - mean_absolute_error: 1.1520 - COD: 2.0557 - NRMSE: 0.1461 - Rsquared: 0.2740 - val_loss: 6.3212 - val_mean_squared_error: 6.3212 - val_mean_absolute_error: 1.5758 - val_COD: 3.0574 - val_NRMSE: 0.2527 - val_Rsquared: -0.2825\n",
      "Epoch 77/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.8690 - mean_squared_error: 2.8690 - mean_absolute_error: 1.1733 - COD: 2.1105 - NRMSE: 0.1478 - Rsquared: 0.2695 - val_loss: 5.7128 - val_mean_squared_error: 5.7128 - val_mean_absolute_error: 1.4897 - val_COD: 3.4880 - val_NRMSE: 0.2372 - val_Rsquared: -0.1304\n",
      "Epoch 78/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.7622 - mean_squared_error: 2.7622 - mean_absolute_error: 1.1463 - COD: 2.2283 - NRMSE: 0.1507 - Rsquared: 0.2892 - val_loss: 5.1490 - val_mean_squared_error: 5.1490 - val_mean_absolute_error: 1.4609 - val_COD: 3.5311 - val_NRMSE: 0.2320 - val_Rsquared: -0.0743\n",
      "Epoch 79/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.2741 - mean_squared_error: 3.2741 - mean_absolute_error: 1.2343 - COD: 2.7546 - NRMSE: 0.1665 - Rsquared: 0.1359 - val_loss: 6.0151 - val_mean_squared_error: 6.0151 - val_mean_absolute_error: 1.5732 - val_COD: 4.8763 - val_NRMSE: 0.2460 - val_Rsquared: -0.2119\n",
      "Epoch 80/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0874 - mean_squared_error: 3.0874 - mean_absolute_error: 1.2349 - COD: 3.9220 - NRMSE: 0.1635 - Rsquared: 0.1793 - val_loss: 5.7023 - val_mean_squared_error: 5.7023 - val_mean_absolute_error: 1.5293 - val_COD: 5.7212 - val_NRMSE: 0.2350 - val_Rsquared: -0.1139\n",
      "Epoch 81/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9018 - mean_squared_error: 2.9018 - mean_absolute_error: 1.1943 - COD: 3.5960 - NRMSE: 0.1532 - Rsquared: 0.2382 - val_loss: 5.3049 - val_mean_squared_error: 5.3049 - val_mean_absolute_error: 1.4817 - val_COD: 4.8463 - val_NRMSE: 0.2267 - val_Rsquared: -0.0353\n",
      "Epoch 82/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7845 - mean_squared_error: 2.7845 - mean_absolute_error: 1.1735 - COD: 3.1212 - NRMSE: 0.1484 - Rsquared: 0.2604 - val_loss: 4.8242 - val_mean_squared_error: 4.8242 - val_mean_absolute_error: 1.4168 - val_COD: 4.4922 - val_NRMSE: 0.2171 - val_Rsquared: 0.0513\n",
      "Epoch 83/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7803 - mean_squared_error: 2.7803 - mean_absolute_error: 1.1659 - COD: 2.8377 - NRMSE: 0.1535 - Rsquared: 0.2745 - val_loss: 4.9851 - val_mean_squared_error: 4.9851 - val_mean_absolute_error: 1.4176 - val_COD: 4.6347 - val_NRMSE: 0.2183 - val_Rsquared: 0.0367\n",
      "Epoch 84/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.3273 - mean_squared_error: 3.3273 - mean_absolute_error: 1.2359 - COD: 3.4498 - NRMSE: 0.1689 - Rsquared: 0.1339 - val_loss: 5.5081 - val_mean_squared_error: 5.5081 - val_mean_absolute_error: 1.4636 - val_COD: 6.6063 - val_NRMSE: 0.2241 - val_Rsquared: -0.0239\n",
      "Epoch 85/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0419 - mean_squared_error: 3.0419 - mean_absolute_error: 1.2061 - COD: 3.9922 - NRMSE: 0.1551 - Rsquared: 0.2214 - val_loss: 4.7460 - val_mean_squared_error: 4.7460 - val_mean_absolute_error: 1.3999 - val_COD: 7.8369 - val_NRMSE: 0.2125 - val_Rsquared: 0.0856\n",
      "Epoch 86/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.9753 - mean_squared_error: 2.9753 - mean_absolute_error: 1.1936 - COD: 3.9465 - NRMSE: 0.1572 - Rsquared: 0.2255 - val_loss: 4.7602 - val_mean_squared_error: 4.7602 - val_mean_absolute_error: 1.4141 - val_COD: 8.5961 - val_NRMSE: 0.2153 - val_Rsquared: 0.0665\n",
      "Epoch 87/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9440 - mean_squared_error: 2.9440 - mean_absolute_error: 1.1849 - COD: 3.4630 - NRMSE: 0.1554 - Rsquared: 0.2218 - val_loss: 4.6966 - val_mean_squared_error: 4.6966 - val_mean_absolute_error: 1.4050 - val_COD: 8.0464 - val_NRMSE: 0.2154 - val_Rsquared: 0.0669\n",
      "Epoch 88/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9196 - mean_squared_error: 2.9196 - mean_absolute_error: 1.1839 - COD: 3.5276 - NRMSE: 0.1548 - Rsquared: 0.2490 - val_loss: 4.6326 - val_mean_squared_error: 4.6326 - val_mean_absolute_error: 1.4242 - val_COD: 7.8712 - val_NRMSE: 0.2176 - val_Rsquared: 0.0523\n",
      "Epoch 89/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.8525 - mean_squared_error: 2.8525 - mean_absolute_error: 1.1644 - COD: 3.3844 - NRMSE: 0.1535 - Rsquared: 0.2664 - val_loss: 4.5417 - val_mean_squared_error: 4.5417 - val_mean_absolute_error: 1.4117 - val_COD: 7.0206 - val_NRMSE: 0.2166 - val_Rsquared: 0.0629\n",
      "Epoch 90/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 3.0525 - mean_squared_error: 3.0525 - mean_absolute_error: 1.2140 - COD: 3.6071 - NRMSE: 0.1592 - Rsquared: 0.1905 - val_loss: 4.6643 - val_mean_squared_error: 4.6643 - val_mean_absolute_error: 1.4208 - val_COD: 7.4244 - val_NRMSE: 0.2167 - val_Rsquared: 0.0590\n",
      "Epoch 91/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.9723 - mean_squared_error: 2.9723 - mean_absolute_error: 1.1952 - COD: 3.6729 - NRMSE: 0.1552 - Rsquared: 0.2394 - val_loss: 4.6854 - val_mean_squared_error: 4.6854 - val_mean_absolute_error: 1.4141 - val_COD: 7.0021 - val_NRMSE: 0.2175 - val_Rsquared: 0.0527\n",
      "Epoch 92/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8502 - mean_squared_error: 2.8502 - mean_absolute_error: 1.1533 - COD: 5.1564 - NRMSE: 0.1471 - Rsquared: 0.2533 - val_loss: 4.6283 - val_mean_squared_error: 4.6283 - val_mean_absolute_error: 1.3896 - val_COD: 6.7580 - val_NRMSE: 0.2161 - val_Rsquared: 0.0649\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7965 - mean_squared_error: 2.7965 - mean_absolute_error: 1.1512 - COD: 2.7722 - NRMSE: 0.1530 - Rsquared: 0.2788 - val_loss: 4.3826 - val_mean_squared_error: 4.3826 - val_mean_absolute_error: 1.3783 - val_COD: 5.0140 - val_NRMSE: 0.2100 - val_Rsquared: 0.1170\n",
      "Epoch 94/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.7868 - mean_squared_error: 2.7868 - mean_absolute_error: 1.1428 - COD: 2.4222 - NRMSE: 0.1483 - Rsquared: 0.2787 - val_loss: 4.2444 - val_mean_squared_error: 4.2444 - val_mean_absolute_error: 1.3615 - val_COD: 5.0427 - val_NRMSE: 0.2088 - val_Rsquared: 0.1290\n",
      "Epoch 95/100\n",
      "899/899 [==============================] - 3s 4ms/step - loss: 2.6726 - mean_squared_error: 2.6726 - mean_absolute_error: 1.1306 - COD: 2.5781 - NRMSE: 0.1427 - Rsquared: 0.3242 - val_loss: 4.5254 - val_mean_squared_error: 4.5254 - val_mean_absolute_error: 1.4060 - val_COD: 5.3102 - val_NRMSE: 0.2145 - val_Rsquared: 0.0799\n",
      "Epoch 96/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.7327 - mean_squared_error: 2.7327 - mean_absolute_error: 1.1370 - COD: 2.7167 - NRMSE: 0.1481 - Rsquared: 0.3014 - val_loss: 4.8094 - val_mean_squared_error: 4.8094 - val_mean_absolute_error: 1.4315 - val_COD: 6.1262 - val_NRMSE: 0.2233 - val_Rsquared: 0.0052\n",
      "Epoch 97/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9959 - mean_squared_error: 2.9959 - mean_absolute_error: 1.1864 - COD: 3.3304 - NRMSE: 0.1555 - Rsquared: 0.2336 - val_loss: 5.0469 - val_mean_squared_error: 5.0469 - val_mean_absolute_error: 1.4882 - val_COD: 9.3715 - val_NRMSE: 0.2300 - val_Rsquared: -0.0541\n",
      "Epoch 98/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 3.0723 - mean_squared_error: 3.0723 - mean_absolute_error: 1.1897 - COD: 3.6422 - NRMSE: 0.1558 - Rsquared: 0.1928 - val_loss: 5.1519 - val_mean_squared_error: 5.1519 - val_mean_absolute_error: 1.4714 - val_COD: 9.5855 - val_NRMSE: 0.2268 - val_Rsquared: -0.0318\n",
      "Epoch 99/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.9789 - mean_squared_error: 2.9789 - mean_absolute_error: 1.1770 - COD: 4.6841 - NRMSE: 0.1572 - Rsquared: 0.2346 - val_loss: 5.0482 - val_mean_squared_error: 5.0482 - val_mean_absolute_error: 1.4560 - val_COD: 8.8621 - val_NRMSE: 0.2258 - val_Rsquared: -0.0209\n",
      "Epoch 100/100\n",
      "899/899 [==============================] - 3s 3ms/step - loss: 2.8783 - mean_squared_error: 2.8783 - mean_absolute_error: 1.1580 - COD: 3.6891 - NRMSE: 0.1566 - Rsquared: 0.2488 - val_loss: 4.8923 - val_mean_squared_error: 4.8923 - val_mean_absolute_error: 1.4351 - val_COD: 6.4979 - val_NRMSE: 0.2261 - val_Rsquared: -0.0202\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 51)          11220     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 51)                21012     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 104       \n",
      "=================================================================\n",
      "Total params: 32,336\n",
      "Trainable params: 32,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Saved ./out/ModelLearner/<MultiAgentEnv instance>_18-07-10_21:07_seqlen100__agentID0_scenario_simple_netDepth2/model0 to model\n",
      "Saved ./out/ModelLearner/<MultiAgentEnv instance>_18-07-10_21:07_seqlen100__agentID0_scenario_simple_netDepth2/model0 to weights\n"
     ]
    }
   ],
   "source": [
    "from MDP_learning.multi_agent import multi, make_env2\n",
    "\n",
    "env_name = 'simple'\n",
    "env = make_env2.make_env(env_name)\n",
    "\n",
    "# Sequence length of 0 uses a feed-forward network\n",
    "MAML = multi.MultiAgentModelLearner(env, mem_size=100000, sequence_length=100, scenario_name=env_name, epochs=100)\n",
    "MAML.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATARI environment\n",
    "Dependencies: https://github.com/keras-rl/keras-rl\n",
    "\n",
    "Tensorboard log will be written to \"./dqn_logs/...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing: <TimeLimit<AtariEnv<PongDeterministic-v4>>>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flat_feat (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 2834567 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: -0.0225\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0227\n",
      "11 episodes - episode_reward: -20.182 [-21.000, -17.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0230\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0213\n",
      "10 episodes - episode_reward: -20.500 [-21.000, -20.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0210\n",
      "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0218\n",
      "11 episodes - episode_reward: -20.182 [-21.000, -19.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: -0.0207\n",
      "10 episodes - episode_reward: -20.100 [-21.000, -19.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: -0.0222\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 0.045 - mean_q: -0.006 - mean_eps: 0.952 - ale.lives: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.058 - mean_q: -0.051 - mean_eps: 0.946 - ale.lives: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0224\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.065 - mean_q: -0.064 - mean_eps: 0.940 - ale.lives: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 203s 20ms/step - reward: -0.0224\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.072 - mean_q: -0.072 - mean_eps: 0.933 - ale.lives: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0225\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.082 - mean_q: -0.084 - mean_eps: 0.927 - ale.lives: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.098 - mean_q: -0.103 - mean_eps: 0.921 - ale.lives: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0228\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.115 - mean_q: -0.122 - mean_eps: 0.914 - ale.lives: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0221\n",
      "11 episodes - episode_reward: -20.273 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.126 - mean_q: -0.137 - mean_eps: 0.908 - ale.lives: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0232\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.137 - mean_q: -0.149 - mean_eps: 0.902 - ale.lives: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0237\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.148 - mean_q: -0.164 - mean_eps: 0.895 - ale.lives: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0221\n",
      "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.164 - mean_q: -0.181 - mean_eps: 0.889 - ale.lives: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0213\n",
      "10 episodes - episode_reward: -20.100 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.174 - mean_q: -0.194 - mean_eps: 0.883 - ale.lives: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0234\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.189 - mean_q: -0.212 - mean_eps: 0.876 - ale.lives: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0214\n",
      "10 episodes - episode_reward: -20.400 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.203 - mean_q: -0.228 - mean_eps: 0.870 - ale.lives: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: -0.0230\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.211 - mean_q: -0.238 - mean_eps: 0.863 - ale.lives: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0229\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.220 - mean_q: -0.249 - mean_eps: 0.857 - ale.lives: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0227\n",
      "12 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.231 - mean_q: -0.261 - mean_eps: 0.851 - ale.lives: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0217\n",
      "10 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.245 - mean_q: -0.280 - mean_eps: 0.844 - ale.lives: 0.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0223\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.256 - mean_q: -0.292 - mean_eps: 0.838 - ale.lives: 0.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0225\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.270 - mean_q: -0.308 - mean_eps: 0.832 - ale.lives: 0.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0232\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.283 - mean_q: -0.324 - mean_eps: 0.825 - ale.lives: 0.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.292 - mean_q: -0.336 - mean_eps: 0.819 - ale.lives: 0.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0238\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.302 - mean_q: -0.347 - mean_eps: 0.813 - ale.lives: 0.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.312 - mean_q: -0.359 - mean_eps: 0.806 - ale.lives: 0.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0221\n",
      "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.328 - mean_q: -0.378 - mean_eps: 0.800 - ale.lives: 0.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0232\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.336 - mean_q: -0.388 - mean_eps: 0.794 - ale.lives: 0.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0228\n",
      "11 episodes - episode_reward: -20.182 [-21.000, -17.000] - loss: 0.012 - mean_absolute_error: 0.348 - mean_q: -0.401 - mean_eps: 0.787 - ale.lives: 0.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0240\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.357 - mean_q: -0.414 - mean_eps: 0.781 - ale.lives: 0.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0224\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.369 - mean_q: -0.428 - mean_eps: 0.775 - ale.lives: 0.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0235\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.377 - mean_q: -0.437 - mean_eps: 0.768 - ale.lives: 0.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.389 - mean_q: -0.453 - mean_eps: 0.762 - ale.lives: 0.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0225\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.407 - mean_q: -0.472 - mean_eps: 0.756 - ale.lives: 0.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.418 - mean_q: -0.485 - mean_eps: 0.749 - ale.lives: 0.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0238\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.422 - mean_q: -0.490 - mean_eps: 0.743 - ale.lives: 0.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0223\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -17.000] - loss: 0.011 - mean_absolute_error: 0.430 - mean_q: -0.500 - mean_eps: 0.736 - ale.lives: 0.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0231\n",
      "12 episodes - episode_reward: -20.417 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.445 - mean_q: -0.518 - mean_eps: 0.730 - ale.lives: 0.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0211\n",
      "10 episodes - episode_reward: -19.900 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.459 - mean_q: -0.535 - mean_eps: 0.724 - ale.lives: 0.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.417 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.470 - mean_q: -0.547 - mean_eps: 0.717 - ale.lives: 0.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0227\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.481 - mean_q: -0.560 - mean_eps: 0.711 - ale.lives: 0.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0227\n",
      "11 episodes - episode_reward: -20.182 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.492 - mean_q: -0.575 - mean_eps: 0.705 - ale.lives: 0.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.498 - mean_q: -0.582 - mean_eps: 0.698 - ale.lives: 0.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0220\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.509 - mean_q: -0.594 - mean_eps: 0.692 - ale.lives: 0.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0235\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.520 - mean_q: -0.607 - mean_eps: 0.686 - ale.lives: 0.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0237\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.538 - mean_q: -0.629 - mean_eps: 0.679 - ale.lives: 0.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.552 - mean_q: -0.646 - mean_eps: 0.673 - ale.lives: 0.000\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: -0.0239\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.563 - mean_q: -0.660 - mean_eps: 0.667 - ale.lives: 0.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0239\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.575 - mean_q: -0.673 - mean_eps: 0.660 - ale.lives: 0.000\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0237\n",
      "12 episodes - episode_reward: -20.417 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.590 - mean_q: -0.691 - mean_eps: 0.654 - ale.lives: 0.000\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.597 - mean_q: -0.699 - mean_eps: 0.648 - ale.lives: 0.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0228\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.602 - mean_q: -0.705 - mean_eps: 0.641 - ale.lives: 0.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0244\n",
      "12 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.605 - mean_q: -0.710 - mean_eps: 0.635 - ale.lives: 0.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.622 - mean_q: -0.730 - mean_eps: 0.629 - ale.lives: 0.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.634 - mean_q: -0.743 - mean_eps: 0.622 - ale.lives: 0.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0238\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.644 - mean_q: -0.756 - mean_eps: 0.616 - ale.lives: 0.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0237\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.653 - mean_q: -0.767 - mean_eps: 0.609 - ale.lives: 0.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.669 - mean_q: -0.787 - mean_eps: 0.603 - ale.lives: 0.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.674 - mean_q: -0.792 - mean_eps: 0.597 - ale.lives: 0.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0242\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.680 - mean_q: -0.799 - mean_eps: 0.590 - ale.lives: 0.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0235\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.681 - mean_q: -0.801 - mean_eps: 0.584 - ale.lives: 0.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0245\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.696 - mean_q: -0.819 - mean_eps: 0.578 - ale.lives: 0.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.703 - mean_q: -0.827 - mean_eps: 0.571 - ale.lives: 0.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0240\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.712 - mean_q: -0.838 - mean_eps: 0.565 - ale.lives: 0.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0234\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.721 - mean_q: -0.849 - mean_eps: 0.559 - ale.lives: 0.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.737 - mean_q: -0.868 - mean_eps: 0.552 - ale.lives: 0.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0230\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.745 - mean_q: -0.878 - mean_eps: 0.546 - ale.lives: 0.000\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0228\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.752 - mean_q: -0.886 - mean_eps: 0.540 - ale.lives: 0.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0229\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.760 - mean_q: -0.895 - mean_eps: 0.533 - ale.lives: 0.000\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0243\n",
      "11 episodes - episode_reward: -20.909 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.768 - mean_q: -0.905 - mean_eps: 0.527 - ale.lives: 0.000\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0237\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.783 - mean_q: -0.923 - mean_eps: 0.521 - ale.lives: 0.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0240\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.795 - mean_q: -0.936 - mean_eps: 0.514 - ale.lives: 0.000\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0245\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.801 - mean_q: -0.944 - mean_eps: 0.508 - ale.lives: 0.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0244\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.815 - mean_q: -0.962 - mean_eps: 0.502 - ale.lives: 0.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0242\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.827 - mean_q: -0.975 - mean_eps: 0.495 - ale.lives: 0.000\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0244\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.838 - mean_q: -0.988 - mean_eps: 0.489 - ale.lives: 0.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0245\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.846 - mean_q: -0.997 - mean_eps: 0.482 - ale.lives: 0.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0225\n",
      "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.863 - mean_q: -1.019 - mean_eps: 0.476 - ale.lives: 0.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0242\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.870 - mean_q: -1.027 - mean_eps: 0.470 - ale.lives: 0.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.874 - mean_q: -1.031 - mean_eps: 0.463 - ale.lives: 0.000\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.881 - mean_q: -1.040 - mean_eps: 0.457 - ale.lives: 0.000\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 0.883 - mean_q: -1.042 - mean_eps: 0.451 - ale.lives: 0.000\n",
      "\n",
      "Interval 88 (870000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0250\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.886 - mean_q: -1.046 - mean_eps: 0.444 - ale.lives: 0.000\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0242\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.889 - mean_q: -1.049 - mean_eps: 0.438 - ale.lives: 0.000\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.898 - mean_q: -1.060 - mean_eps: 0.432 - ale.lives: 0.000\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0235\n",
      "12 episodes - episode_reward: -20.333 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.903 - mean_q: -1.066 - mean_eps: 0.425 - ale.lives: 0.000\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0227\n",
      "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.906 - mean_q: -1.070 - mean_eps: 0.419 - ale.lives: 0.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.907 - mean_q: -1.072 - mean_eps: 0.413 - ale.lives: 0.000\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.916 - mean_q: -1.082 - mean_eps: 0.406 - ale.lives: 0.000\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0240\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.921 - mean_q: -1.088 - mean_eps: 0.400 - ale.lives: 0.000\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0240\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.925 - mean_q: -1.092 - mean_eps: 0.394 - ale.lives: 0.000\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0236\n",
      "12 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.934 - mean_q: -1.104 - mean_eps: 0.387 - ale.lives: 0.000\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.949 - mean_q: -1.122 - mean_eps: 0.381 - ale.lives: 0.000\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.963 - mean_q: -1.138 - mean_eps: 0.375 - ale.lives: 0.000\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.974 - mean_q: -1.152 - mean_eps: 0.368 - ale.lives: 0.000\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.983 - mean_q: -1.162 - mean_eps: 0.362 - ale.lives: 0.000\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.982 - mean_q: -1.161 - mean_eps: 0.355 - ale.lives: 0.000\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.985 - mean_q: -1.165 - mean_eps: 0.349 - ale.lives: 0.000\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0233\n",
      "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.993 - mean_q: -1.174 - mean_eps: 0.343 - ale.lives: 0.000\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.417 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.004 - mean_q: -1.188 - mean_eps: 0.336 - ale.lives: 0.000\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.010 - mean_q: -1.195 - mean_eps: 0.330 - ale.lives: 0.000\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0243\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.014 - mean_q: -1.199 - mean_eps: 0.324 - ale.lives: 0.000\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0240\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 1.019 - mean_q: -1.206 - mean_eps: 0.317 - ale.lives: 0.000\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.029 - mean_q: -1.216 - mean_eps: 0.311 - ale.lives: 0.000\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0245\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.039 - mean_q: -1.229 - mean_eps: 0.305 - ale.lives: 0.000\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0251\n",
      "13 episodes - episode_reward: -20.692 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.044 - mean_q: -1.235 - mean_eps: 0.298 - ale.lives: 0.000\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.050 - mean_q: -1.241 - mean_eps: 0.292 - ale.lives: 0.000\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0236\n",
      "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.056 - mean_q: -1.250 - mean_eps: 0.286 - ale.lives: 0.000\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0242\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.066 - mean_q: -1.261 - mean_eps: 0.279 - ale.lives: 0.000\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.073 - mean_q: -1.270 - mean_eps: 0.273 - ale.lives: 0.000\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.081 - mean_q: -1.280 - mean_eps: 0.267 - ale.lives: 0.000\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0235\n",
      "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 1.082 - mean_q: -1.279 - mean_eps: 0.260 - ale.lives: 0.000\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0255\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.087 - mean_q: -1.286 - mean_eps: 0.254 - ale.lives: 0.000\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.092 - mean_q: -1.292 - mean_eps: 0.248 - ale.lives: 0.000\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0238\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 1.095 - mean_q: -1.296 - mean_eps: 0.241 - ale.lives: 0.000\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.100 - mean_q: -1.303 - mean_eps: 0.235 - ale.lives: 0.000\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0259\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.112 - mean_q: -1.317 - mean_eps: 0.228 - ale.lives: 0.000\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.120 - mean_q: -1.326 - mean_eps: 0.222 - ale.lives: 0.000\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0250\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.125 - mean_q: -1.332 - mean_eps: 0.216 - ale.lives: 0.000\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.133 - mean_q: -1.342 - mean_eps: 0.209 - ale.lives: 0.000\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.136 - mean_q: -1.346 - mean_eps: 0.203 - ale.lives: 0.000\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0251\n",
      "13 episodes - episode_reward: -20.692 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.141 - mean_q: -1.352 - mean_eps: 0.197 - ale.lives: 0.000\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.148 - mean_q: -1.359 - mean_eps: 0.190 - ale.lives: 0.000\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0250\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.149 - mean_q: -1.362 - mean_eps: 0.184 - ale.lives: 0.000\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0245\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.155 - mean_q: -1.369 - mean_eps: 0.178 - ale.lives: 0.000\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.159 - mean_q: -1.374 - mean_eps: 0.171 - ale.lives: 0.000\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.165 - mean_q: -1.378 - mean_eps: 0.165 - ale.lives: 0.000\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0255\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.167 - mean_q: -1.384 - mean_eps: 0.159 - ale.lives: 0.000\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.171 - mean_q: -1.387 - mean_eps: 0.152 - ale.lives: 0.000\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.179 - mean_q: -1.397 - mean_eps: 0.146 - ale.lives: 0.000\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.190 - mean_q: -1.411 - mean_eps: 0.140 - ale.lives: 0.000\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0245\n",
      "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.208 - mean_q: -1.431 - mean_eps: 0.133 - ale.lives: 0.000\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.206 - mean_q: -1.428 - mean_eps: 0.127 - ale.lives: 0.000\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 203s 20ms/step - reward: -0.0265\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.201 - mean_q: -1.423 - mean_eps: 0.121 - ale.lives: 0.000\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.203 - mean_q: -1.425 - mean_eps: 0.114 - ale.lives: 0.000\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0262\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.215 - mean_q: -1.440 - mean_eps: 0.108 - ale.lives: 0.000\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0247\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.222 - mean_q: -1.448 - mean_eps: 0.102 - ale.lives: 0.000\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.225 - mean_q: -1.452 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.229 - mean_q: -1.457 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.231 - mean_q: -1.459 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0257\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 1.239 - mean_q: -1.469 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.251 - mean_q: -1.482 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.256 - mean_q: -1.489 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.267 - mean_q: -1.501 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0258\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.270 - mean_q: -1.506 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0242\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.273 - mean_q: -1.509 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.276 - mean_q: -1.513 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.282 - mean_q: -1.520 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.277 - mean_q: -1.515 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0261\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.275 - mean_q: -1.512 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.289 - mean_q: -1.529 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0264\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.290 - mean_q: -1.528 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.298 - mean_q: -1.538 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0262\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.300 - mean_q: -1.542 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.302 - mean_q: -1.545 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0265\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.302 - mean_q: -1.544 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0262\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.303 - mean_q: -1.544 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.302 - mean_q: -1.543 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.301 - mean_q: -1.542 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0264\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.302 - mean_q: -1.544 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0263\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.303 - mean_q: -1.545 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.306 - mean_q: -1.549 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.309 - mean_q: -1.552 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.313 - mean_q: -1.557 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.315 - mean_q: -1.559 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0259\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.321 - mean_q: -1.565 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0252\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.324 - mean_q: -1.571 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.328 - mean_q: -1.575 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0256\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.331 - mean_q: -1.580 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0263\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.336 - mean_q: -1.584 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.341 - mean_q: -1.591 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.348 - mean_q: -1.599 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.352 - mean_q: -1.604 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0248\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.358 - mean_q: -1.611 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.369 - mean_q: -1.624 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0254\n",
      "13 episodes - episode_reward: -20.692 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.381 - mean_q: -1.637 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.391 - mean_q: -1.650 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.383 - mean_q: -1.642 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.398 - mean_q: -1.659 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.410 - mean_q: -1.674 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.421 - mean_q: -1.687 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0253\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.434 - mean_q: -1.701 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.441 - mean_q: -1.711 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.448 - mean_q: -1.718 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.458 - mean_q: -1.731 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0246\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.471 - mean_q: -1.746 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0257\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.480 - mean_q: -1.756 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0260\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.486 - mean_q: -1.764 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.492 - mean_q: -1.772 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0258\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.492 - mean_q: -1.771 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0260\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.492 - mean_q: -1.772 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.494 - mean_q: -1.773 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.499 - mean_q: -1.780 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.505 - mean_q: -1.788 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0263\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.508 - mean_q: -1.791 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 201 (2000000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.512 - mean_q: -1.796 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 202 (2010000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0268\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.517 - mean_q: -1.801 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 203 (2020000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 1.523 - mean_q: -1.808 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 204 (2030000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0254\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.533 - mean_q: -1.820 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 205 (2040000 steps performed)\n",
      "10000/10000 [==============================] - 207s 21ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 1.540 - mean_q: -1.829 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 206 (2050000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.543 - mean_q: -1.832 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 207 (2060000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.546 - mean_q: -1.836 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 208 (2070000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.544 - mean_q: -1.834 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 209 (2080000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0259\n",
      "13 episodes - episode_reward: -20.769 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.541 - mean_q: -1.829 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 210 (2090000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.538 - mean_q: -1.826 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 211 (2100000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.538 - mean_q: -1.825 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 212 (2110000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.013 - mean_absolute_error: 1.539 - mean_q: -1.827 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 213 (2120000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.539 - mean_q: -1.827 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 214 (2130000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.540 - mean_q: -1.830 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 215 (2140000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0264\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.543 - mean_q: -1.832 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 216 (2150000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.548 - mean_q: -1.839 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 217 (2160000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0261\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.555 - mean_q: -1.846 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 218 (2170000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0259\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.554 - mean_q: -1.846 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 219 (2180000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.564 - mean_q: -1.857 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 220 (2190000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.571 - mean_q: -1.866 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 221 (2200000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0261\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.574 - mean_q: -1.870 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 222 (2210000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.575 - mean_q: -1.871 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 223 (2220000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.572 - mean_q: -1.867 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 224 (2230000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0259\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.579 - mean_q: -1.876 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 225 (2240000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.585 - mean_q: -1.883 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 226 (2250000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0263\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.596 - mean_q: -1.895 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 227 (2260000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.602 - mean_q: -1.903 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 228 (2270000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0264\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.610 - mean_q: -1.913 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 229 (2280000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.615 - mean_q: -1.918 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 230 (2290000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0254\n",
      "13 episodes - episode_reward: -20.692 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 1.624 - mean_q: -1.930 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 231 (2300000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0250\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.626 - mean_q: -1.932 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 232 (2310000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.628 - mean_q: -1.935 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 233 (2320000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.638 - mean_q: -1.945 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 234 (2330000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0271\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.632 - mean_q: -1.939 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 235 (2340000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.631 - mean_q: -1.938 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 236 (2350000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.631 - mean_q: -1.938 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 237 (2360000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 1.631 - mean_q: -1.939 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 238 (2370000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.638 - mean_q: -1.947 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 239 (2380000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.642 - mean_q: -1.952 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 240 (2390000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0261\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.649 - mean_q: -1.959 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 241 (2400000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0262\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.012 - mean_absolute_error: 1.646 - mean_q: -1.957 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 242 (2410000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0256\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.642 - mean_q: -1.951 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 243 (2420000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.637 - mean_q: -1.944 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 244 (2430000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.638 - mean_q: -1.946 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 245 (2440000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.640 - mean_q: -1.948 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 246 (2450000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0259\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.635 - mean_q: -1.943 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 247 (2460000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0258\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.637 - mean_q: -1.945 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 248 (2470000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0239\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.639 - mean_q: -1.946 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 249 (2480000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0265\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.639 - mean_q: -1.946 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 250 (2490000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0244\n",
      "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.632 - mean_q: -1.939 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 251 (2500000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0254\n",
      "13 episodes - episode_reward: -20.692 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.628 - mean_q: -1.932 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 252 (2510000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.628 - mean_q: -1.934 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 253 (2520000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0260\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.629 - mean_q: -1.936 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 254 (2530000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0258\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.633 - mean_q: -1.940 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 255 (2540000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.630 - mean_q: -1.937 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 256 (2550000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: -0.0264\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.013 - mean_absolute_error: 1.627 - mean_q: -1.932 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 257 (2560000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0260\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.631 - mean_q: -1.938 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 258 (2570000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.639 - mean_q: -1.947 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 259 (2580000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0259\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.643 - mean_q: -1.953 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 260 (2590000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0253\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.642 - mean_q: -1.950 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 261 (2600000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: -0.0247\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.637 - mean_q: -1.945 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 262 (2610000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0262\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.637 - mean_q: -1.945 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 263 (2620000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0249\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 1.637 - mean_q: -1.944 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 264 (2630000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: -0.0254\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.635 - mean_q: -1.943 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 265 (2640000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0258\n",
      "12 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.013 - mean_absolute_error: 1.642 - mean_q: -1.950 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 266 (2650000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0263\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.642 - mean_q: -1.952 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 267 (2660000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0253\n",
      "13 episodes - episode_reward: -20.923 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.648 - mean_q: -1.958 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 268 (2670000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.670 - mean_q: -1.983 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 269 (2680000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.664 - mean_q: -1.977 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 270 (2690000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.666 - mean_q: -1.979 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 271 (2700000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: -0.0260\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.013 - mean_absolute_error: 1.668 - mean_q: -1.982 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 272 (2710000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0251\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.675 - mean_q: -1.992 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 273 (2720000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0244\n",
      "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.014 - mean_absolute_error: 1.671 - mean_q: -1.985 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 274 (2730000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0256\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.671 - mean_q: -1.985 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 275 (2740000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.669 - mean_q: -1.983 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 276 (2750000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 1.674 - mean_q: -1.989 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 277 (2760000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: -0.0255\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.672 - mean_q: -1.987 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 278 (2770000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: -0.0261\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.676 - mean_q: -1.991 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 279 (2780000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: -0.0262\n",
      "13 episodes - episode_reward: -20.846 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.677 - mean_q: -1.994 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 280 (2790000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: -0.0263\n",
      "13 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.013 - mean_absolute_error: 1.676 - mean_q: -1.992 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 281 (2800000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: -0.0257\n",
      "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.680 - mean_q: -1.996 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 282 (2810000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0252\n",
      "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 1.675 - mean_q: -1.990 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 283 (2820000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: -0.0254\n",
      "12 episodes - episode_reward: -20.750 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 1.677 - mean_q: -1.992 - mean_eps: 0.100 - ale.lives: 0.000\n",
      "\n",
      "Interval 284 (2830000 steps performed)\n",
      " 4565/10000 [============>.................] - ETA: 1:53 - reward: -0.0237done, took 59084.940 seconds\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_state (InputLayer)          (None, 1, 3136)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat_state (Flatten)            (None, 3136)         0           enc_state[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_act (Flatten)              (None, 1)            0           action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoded_state_and_action (Conca (None, 3137)         0           flat_state[0][0]                 \n",
      "                                                                 flat_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2048)         6426624     encoded_state_and_action[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2048)         4196352     dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         2098176     dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predicted_next_state (Dense)    (None, 3136)         3214400     dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predicted_reward (Dense)        (None, 1)            1025        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predicted_terminal (Dense)      (None, 1)            1025        dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,937,602\n",
      "Trainable params: 15,937,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "logging to PongDeterministic-v4_slen1_lwidth2048-1531316454.0039704\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flat_feat (Flatten)          (None, 3136)              0         \n",
      "=================================================================\n",
      "Total params: 77,984\n",
      "Trainable params: 77,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1133826 samples, validate on 283457 samples\n",
      "Epoch 1/3\n",
      "1133826/1133826 [==============================] - 2830s 2ms/step - loss: 0.0244 - predicted_next_state_loss: 1.1908e-06 - predicted_reward_loss: 0.0243 - predicted_terminal_loss: 1.0031e-04 - predicted_next_state_mean_squared_error: 1.1908e-06 - predicted_next_state_mean_absolute_error: 9.7458e-05 - predicted_next_state_COD: 1.5657 - predicted_next_state_NRMSE: 1071.8254 - predicted_next_state_Rsquared: -380.0426 - predicted_reward_mean_squared_error: 0.0243 - predicted_reward_mean_absolute_error: 0.0484 - predicted_reward_COD: 7389305.5151 - predicted_reward_NRMSE: 111615.6413 - predicted_reward_Rsquared: -104962.9349 - predicted_terminal_mean_squared_error: 2.3431e-05 - predicted_terminal_mean_absolute_error: 8.3781e-05 - predicted_terminal_COD: 23.3439 - predicted_terminal_NRMSE: 1146.1323 - predicted_terminal_Rsquared: -7496.8997 - val_loss: 0.0240 - val_predicted_next_state_loss: 8.6083e-15 - val_predicted_reward_loss: 0.0240 - val_predicted_terminal_loss: 3.8443e-07 - val_predicted_next_state_mean_squared_error: 8.6083e-15 - val_predicted_next_state_mean_absolute_error: 6.7426e-08 - val_predicted_next_state_COD: 2.7546e-06 - val_predicted_next_state_NRMSE: 0.6743 - val_predicted_next_state_Rsquared: 1.0000 - val_predicted_reward_mean_squared_error: 0.0240 - val_predicted_reward_mean_absolute_error: 0.0430 - val_predicted_reward_COD: 7693249.5209 - val_predicted_reward_NRMSE: 86595.1739 - val_predicted_reward_Rsquared: -53674.4923 - val_predicted_terminal_mean_squared_error: 1.4779e-13 - val_predicted_terminal_mean_absolute_error: 3.8443e-07 - val_predicted_terminal_COD: 4.7293e-05 - val_predicted_terminal_NRMSE: 3.8443 - val_predicted_terminal_Rsquared: 1.0000\n",
      "Epoch 2/3\n",
      "1133826/1133826 [==============================] - 2815s 2ms/step - loss: 0.0242 - predicted_next_state_loss: 7.7778e-13 - predicted_reward_loss: 0.0242 - predicted_terminal_loss: 1.1614e-07 - predicted_next_state_mean_squared_error: 7.7778e-13 - predicted_next_state_mean_absolute_error: 1.4020e-07 - predicted_next_state_COD: 2.4889e-04 - predicted_next_state_NRMSE: 1.4020 - predicted_next_state_Rsquared: 0.9998 - predicted_reward_mean_squared_error: 0.0242 - predicted_reward_mean_absolute_error: 0.0479 - predicted_reward_COD: 7730132.5205 - predicted_reward_NRMSE: 109207.0738 - predicted_reward_Rsquared: -87195.3576 - predicted_terminal_mean_squared_error: 1.6265e-14 - predicted_terminal_mean_absolute_error: 1.1584e-07 - predicted_terminal_COD: 5.2049e-06 - predicted_terminal_NRMSE: 1.1584 - predicted_terminal_Rsquared: 1.0000 - val_loss: 0.0240 - val_predicted_next_state_loss: 0.0000e+00 - val_predicted_reward_loss: 0.0240 - val_predicted_terminal_loss: 1.0000e-07 - val_predicted_next_state_mean_squared_error: 0.0000e+00 - val_predicted_next_state_mean_absolute_error: 4.5888e-32 - val_predicted_next_state_COD: 0.0000e+00 - val_predicted_next_state_NRMSE: 0.0000e+00 - val_predicted_next_state_Rsquared: 1.0000 - val_predicted_reward_mean_squared_error: 0.0240 - val_predicted_reward_mean_absolute_error: 0.0527 - val_predicted_reward_COD: 7694297.3259 - val_predicted_reward_NRMSE: 131959.5397 - val_predicted_reward_Rsquared: -124642.0548 - val_predicted_terminal_mean_squared_error: 9.9324e-15 - val_predicted_terminal_mean_absolute_error: 9.9661e-08 - val_predicted_terminal_COD: 3.1784e-06 - val_predicted_terminal_NRMSE: 0.9966 - val_predicted_terminal_Rsquared: 1.0000\n",
      "Epoch 3/3\n",
      "1133826/1133826 [==============================] - 2814s 2ms/step - loss: 0.0242 - predicted_next_state_loss: 0.0000e+00 - predicted_reward_loss: 0.0242 - predicted_terminal_loss: 1.0000e-07 - predicted_next_state_mean_squared_error: 0.0000e+00 - predicted_next_state_mean_absolute_error: 4.6172e-32 - predicted_next_state_COD: 0.0000e+00 - predicted_next_state_NRMSE: 0.0000e+00 - predicted_next_state_Rsquared: 1.0000 - predicted_reward_mean_squared_error: 0.0242 - predicted_reward_mean_absolute_error: 0.0480 - predicted_reward_COD: 7730009.3841 - predicted_reward_NRMSE: 109328.2657 - predicted_reward_Rsquared: -87327.7189 - predicted_terminal_mean_squared_error: 9.9324e-15 - predicted_terminal_mean_absolute_error: 9.9661e-08 - predicted_terminal_COD: 3.1784e-06 - predicted_terminal_NRMSE: 0.9966 - predicted_terminal_Rsquared: 1.0000 - val_loss: 0.0240 - val_predicted_next_state_loss: 0.0000e+00 - val_predicted_reward_loss: 0.0240 - val_predicted_terminal_loss: 1.0000e-07 - val_predicted_next_state_mean_squared_error: 0.0000e+00 - val_predicted_next_state_mean_absolute_error: 4.6240e-32 - val_predicted_next_state_COD: 0.0000e+00 - val_predicted_next_state_NRMSE: 0.0000e+00 - val_predicted_next_state_Rsquared: 1.0000 - val_predicted_reward_mean_squared_error: 0.0240 - val_predicted_reward_mean_absolute_error: 0.0427 - val_predicted_reward_COD: 7694466.3571 - val_predicted_reward_NRMSE: 84928.4814 - val_predicted_reward_Rsquared: -51628.2200 - val_predicted_terminal_mean_squared_error: 9.9324e-15 - val_predicted_terminal_mean_absolute_error: 9.9661e-08 - val_predicted_terminal_COD: 3.1784e-06 - val_predicted_terminal_NRMSE: 0.9966 - val_predicted_terminal_Rsquared: 1.0000\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "action_input (InputLayer)       (None, 4, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_state (InputLayer)          (None, 4, 3136)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 4, 1)         0           action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoded_state_and_action (Conca (None, 4, 3137)      0           enc_state[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 2048)         42483712    encoded_state_and_action[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "predicted_next_state (Dense)    (None, 3136)         6425664     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "predicted_reward (Dense)        (None, 1)            2049        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "predicted_terminal (Dense)      (None, 1)            2049        lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 48,913,474\n",
      "Trainable params: 48,913,474\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "logging to PongDeterministic-v4_slen4_lwidth2048-1531331140.9575112\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 4, 84, 84)         0         \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flat_feat (Flatten)          (None, 3136)              0         \n",
      "=================================================================\n",
      "Total params: 77,984\n",
      "Trainable params: 77,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-af486845ddc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mhstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         layer_width=2048)\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/ModelRL/MDP_learning/from_pixels/dqn_kerasrl_modellearn.py\u001b[0m in \u001b[0;36mtrainML\u001b[0;34m(cfg, dqn, sequence_length, hstate_size, layer_width)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mn_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mhstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mnext_hstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from MDP_learning.from_pixels import dqn_kerasrl_modellearn\n",
    "import gym\n",
    "\n",
    "env_name = 'PongDeterministic-v4'\n",
    "cfg = dqn_kerasrl_modellearn.AtariConfig(env_name)\n",
    "environment = gym.make(cfg.env_name)\n",
    "print('Playing: {}'.format(environment))\n",
    "num_actions = environment.action_space.n\n",
    "\n",
    "processor = dqn_kerasrl_modellearn.AtariProcessor(cfg.INPUT_SHAPE)\n",
    "dqn_agent, hidden_state_size = dqn_kerasrl_modellearn.setupDQN(cfg, num_actions, processor)\n",
    "\n",
    "dqn_kerasrl_modellearn.trainDQN(cfg, environment, dqn_agent)\n",
    "\n",
    "for seq_len in [1, 4, 16]:\n",
    "    dynamics_model, dqn_convolutions = dqn_kerasrl_modellearn.trainML(\n",
    "        cfg, dqn_agent,\n",
    "        sequence_length=seq_len,\n",
    "        hstate_size=hidden_state_size,\n",
    "        layer_width=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
